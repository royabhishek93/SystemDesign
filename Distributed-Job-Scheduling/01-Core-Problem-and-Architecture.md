# Core Problem and Architecture

**Target Level**: Senior+ (5-7 years)  
**Interview Focus**: Understanding distributed systems fundamentals

## üìã Always Start Here in Interviews

Before jumping to solutions, demonstrate you understand the problem deeply. This signals senior-level thinking.

## ‚ùå Why Normal Scheduling Fails

### Scenario: E-commerce Daily Report Generation

**Setup**:
- Spring Boot microservice
- 5 EC2 instances behind load balancer
- Job: Generate daily sales report at 2:00 AM

**Code everyone writes** (‚ùå Wrong in distributed systems):

```java
@Component
public class ReportScheduler {
    
    @Scheduled(cron = "0 0 2 * * *")  // Run at 2:00 AM daily
    public void generateDailyReport() {
        List<Order> orders = orderRepository.findByDate(yesterday);
        Report report = reportService.generate(orders);
        emailService.send(report);
    }
}
```

### What Actually Happens

```
2:00 AM:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Instance 1  ‚îÇ ‚Üí Generates report ‚Üí Sends email
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Instance 2  ‚îÇ ‚Üí Generates report ‚Üí Sends email
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Instance 3  ‚îÇ ‚Üí Generates report ‚Üí Sends email
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Instance 4  ‚îÇ ‚Üí Generates report ‚Üí Sends email
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Instance 5  ‚îÇ ‚Üí Generates report ‚Üí Sends email
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Result: ‚ùå Customer receives 5 identical emails
        ‚ùå Database queried 5 times (performance hit)
        ‚ùå CPU/memory wasted on duplicate work
```

## üéØ The Root Cause (Interview Gold)

**Say this line in interview**:

> "In a distributed system, each instance has its own JVM with its own scheduler. Spring's `@Scheduled` annotation is process-local, not cluster-aware. Without external coordination, all instances independently execute the same schedule."

This shows you understand:
- Process vs cluster scope
- Lack of built-in coordination
- Need for external mechanism

## üèóÔ∏è High-Level Architecture Pattern

### What We're Building

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Multiple Service Instances (N nodes)            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Node 1  ‚îÇ  ‚îÇ Node 2  ‚îÇ  ‚îÇ Node 3  ‚îÇ  ‚îÇ Node N  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ           ‚îÇ            ‚îÇ            ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Coordination Layer   ‚îÇ
        ‚îÇ  (Lock / Election)    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
             Only One Node
             Executes Job
                    ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ   Job Processing      ‚îÇ
        ‚îÇ   (Business Logic)    ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ  Retry / Monitoring   ‚îÇ
        ‚îÇ  Failure Recovery     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîë Key Components (Mention These in Interview)

### 1. Job Scheduler
**Purpose**: Triggers execution attempt at scheduled time  
**Scope**: Runs on all instances  
**Tech**: Spring `@Scheduled`, Quartz, cron

### 2. Coordination Mechanism ‚≠ê (Most Critical)
**Purpose**: Ensures only one instance proceeds  
**Options**:
- Distributed lock (Redis, Database)
- Leader election (Kubernetes, ZooKeeper, etcd)
- Persistent job store (Quartz clustered)

**This is what the interview is testing!**

### 3. Execution Worker
**Purpose**: Runs actual business logic  
**Requirements**: Must be idempotent  
**Monitoring**: Track start/end, success/failure

### 4. Failure Recovery
**Purpose**: Handle crashed nodes, retry failed jobs  
**Mechanisms**: TTL, lease expiry, health checks

### 5. State Persistence
**Purpose**: Track job execution history  
**Storage**: Database, Redis  
**Statuses**: `PENDING ‚Üí RUNNING ‚Üí COMPLETED ‚Üí FAILED`

## üí° Two Fundamental Approaches (Senior Must Know)

### Approach A: Distributed Lock Pattern

**Concept**: Racing model - all nodes attempt, winner executes

```
Time 2:00 AM:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Instance 1  ‚îÇ ‚Üí Try lock ‚Üí ‚úÖ Got lock ‚Üí Run job ‚Üí Release
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Instance 2  ‚îÇ ‚Üí Try lock ‚Üí ‚ùå Locked ‚Üí Skip
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Instance 3  ‚îÇ ‚Üí Try lock ‚Üí ‚ùå Locked ‚Üí Skip
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Characteristics**:
- ‚úÖ Simple to understand
- ‚úÖ Works with any scheduler
- ‚ö†Ô∏è All nodes wake up and compete
- ‚ö†Ô∏è Coordination overhead on every execution

**Best for**: Simple systems, infrequent jobs

### Approach B: Leader Election Pattern

**Concept**: One leader node owns scheduling responsibility

```
Startup:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Instance 1  ‚îÇ ‚Üí Elected Leader ‚úÖ ‚Üí Runs all scheduled jobs
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Instance 2  ‚îÇ ‚Üí Follower ‚Üí Does nothing
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Instance 3  ‚îÇ ‚Üí Follower ‚Üí Does nothing
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

If Instance 1 crashes ‚Üí Instance 2 becomes leader
```

**Characteristics**:
- ‚úÖ Zero coordination overhead during execution
- ‚úÖ Scales to many jobs
- ‚úÖ Clearer ownership model
- ‚ö†Ô∏è More complex to implement
- ‚ö†Ô∏è Leader becomes single point (until failover)

**Best for**: Many scheduled jobs, large clusters

## üéØ Interview Statement Template

**When asked "How do you handle distributed scheduling?"**

> "In distributed systems, I use external coordination to ensure single execution. For simple cases, I implement a distributed lock pattern where nodes compete before executing. For larger scale or many jobs, I prefer leader election where one designated node owns the scheduling responsibility. Both approaches require idempotent job design and failure recovery mechanisms like TTL for lock expiry or lease-based leader election."

**Why this answer works**:
- ‚úÖ Mentions two approaches (shows breadth)
- ‚úÖ Explains when to use each (tradeoffs)
- ‚úÖ Mentions critical concepts (idempotency, failure)
- ‚úÖ Avoids buzzwords without explanation

## ‚ùå Wrong vs ‚úÖ Right Examples

### Question: "How do you prevent duplicate execution?"

**‚ùå Wrong Answer** (Junior Level):
> "I use Redis to store a flag and check before running."

**Why wrong**: Too vague, no details on race conditions, TTL, failure handling

**‚úÖ Right Answer** (Senior Level):
> "I use Redis distributed lock with `SETNX` command and TTL. Before job execution, each instance attempts to acquire the lock atomically. Only the instance that successfully sets the key proceeds. The TTL prevents deadlock if the instance crashes. After completion, I explicitly release the lock. I also implement idempotent processing since distributed systems can't guarantee exactly-once execution."

**Why right**: Specific technology, mentions atomicity, handles failures, shows systems thinking

### Question: "What if your Redis instance goes down?"

**‚ùå Wrong Answer**:
> "We have Redis cluster for high availability."

**Why wrong**: Doesn't address split-brain scenarios or degradation strategy

**‚úÖ Right Answer**:
> "Redis cluster provides availability, but we also implement circuit breaker pattern. If Redis is unavailable, we can either fail-safe by not running the job, or degrade to running on single instance with external alerting. For critical jobs, I prefer using a strongly consistent coordination service like ZooKeeper or etcd, which handles network partitions better through quorum-based consensus. Alternatively, database-based locking with `SELECT FOR UPDATE` provides strong consistency using existing infrastructure."

**Why right**: Multiple layers, graceful degradation, mentions CAP theorem concepts

## üß† Deep Explanation - Why This Is Hard

### The Distributed Systems Trilemma

You want:
1. **Single execution** (correctness)
2. **High availability** (always runs even if nodes fail)
3. **No coordination overhead** (performance)

**You can only pick 2!**

This is why different approaches exist for different requirements.

### Race Condition Example

```java
// ‚ùå This doesn't work!
@Scheduled(cron = "0 0 2 * * *")
public void generateReport() {
    Boolean executed = redis.get("report:executed");
    
    if (!executed) {  // ‚Üê Race condition here!
        // 10ms gap - another instance can enter
        redis.set("report:executed", true);
        doActualWork();
    }
}
```

**What happens**:
```
Time 0ms:  Instance 1 checks ‚Üí null ‚Üí proceeds
Time 5ms:  Instance 2 checks ‚Üí null ‚Üí proceeds (race!)
Time 10ms: Instance 1 sets flag
Time 11ms: Instance 2 sets flag
Result: Both execute ‚ùå
```

**Fix**: Use atomic operation
```java
// ‚úÖ Atomic check-and-set
Boolean locked = redis.setIfAbsent(
    "report:lock", 
    "instance-1", 
    Duration.ofMinutes(5)  // TTL for crash recovery
);
```

## üìä Critical Pitfalls (Interview Follow-Up Gold)

### Pitfall #1: Lock Without TTL

**Problem**: Instance crashes while holding lock ‚Üí deadlock

```java
// ‚ùå Dangerous
redis.set("lock", "true");
doWork();
redis.delete("lock");  // Never reaches if crash!
```

**Solution**: Always use TTL
```java
// ‚úÖ Safe
redis.setex("lock", 300, "true");  // 5 min expiry
```

### Pitfall #2: TTL Too Short

**Problem**: Job runs longer than TTL ‚Üí lock expires ‚Üí another instance starts

```
0:00 - Instance 1 acquires lock (TTL=5 min)
0:03 - Still processing...
0:05 - Lock expires ‚ùå
0:05 - Instance 2 acquires lock
Result: Both running simultaneously!
```

**Solution**: TTL > max job duration, or implement lock renewal (watchdog)

### Pitfall #3: Non-Idempotent Jobs

**Problem**: Even with locking, network issues or retries can cause duplicate execution

**Solution**: Always design for idempotency (covered in Chapter 03)

## üî• Follow-Up Questions & Answers

### Q1: "Can't you just run the scheduler on one instance?"

**‚úÖ Answer**: 
"Yes, but that creates a single point of failure. If that instance crashes, no jobs run until manual intervention or auto-scaling replaces it. In production, we want automatic failover. Distributed coordination provides both single execution and high availability."

### Q2: "Why not use a sticky session to ensure one instance handles it?"

**‚úÖ Answer**:
"Sticky sessions work for user requests from load balancers, but scheduled jobs aren't triggered by external requests. Each instance's JVM scheduler fires independently. Sticky sessions can't prevent internal timed triggers from firing on all nodes."

### Q3: "What's the performance impact of coordination?"

**‚úÖ Answer**:
"With distributed locks, all instances attempt acquisition on every execution - this creates N network calls to Redis/DB per job trigger. With leader election, only one node runs the scheduler, so zero coordination overhead per job. However, leader election adds complexity during startup and failover. For infrequent jobs (hourly/daily), lock overhead is negligible. For frequent jobs (every second), leader election scales better."

### Q4: "How do you handle time zone differences in global deployments?"

**‚úÖ Answer**:
"Scheduled jobs should use UTC internally to avoid DST issues. If business requirements need local time (e.g., 'send email at 9 AM user local time'), I convert to a queue-based approach: a UTC scheduler pushes user IDs to a queue partitioned by timezone, then workers process each partition at the appropriate UTC time."

### Q5: "What if job takes 2 hours but runs hourly?"

**‚úÖ Answer**:
"This creates job overlap risk. Solutions: 1) Check if previous execution is still running before starting new one (using status in DB). 2) Use a queue-based pattern where scheduler just enqueues work and workers process it sequentially. 3) Increase execution interval. 4) Optimize the job to run faster, potentially by partitioning work across parallel workers."

## ‚úÖ Quick Checklist for Interviews

When explaining architecture, make sure you mention:

- [ ] Why process-local scheduling fails
- [ ] External coordination requirement (lock or election)
- [ ] Atomic operations to prevent race conditions
- [ ] TTL for failure recovery
- [ ] Idempotent job design
- [ ] Failure recovery strategy
- [ ] At least two implementation approaches
- [ ] Tradeoffs between approaches

## üìà Complexity Comparison

| Approach | Setup Complexity | Runtime Overhead | Failure Recovery | Scale |
|----------|-----------------|------------------|------------------|-------|
| No coordination (broken) | 1/5 ‚≠ê | 1/5 ‚≠ê | 0/5 | 5/5 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Distributed lock | 2/5 ‚≠ê‚≠ê | 3/5 ‚≠ê‚≠ê‚≠ê | 4/5 ‚≠ê‚≠ê‚≠ê‚≠ê | 3/5 ‚≠ê‚≠ê‚≠ê |
| Leader election | 4/5 ‚≠ê‚≠ê‚≠ê‚≠ê | 1/5 ‚≠ê | 5/5 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 5/5 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| Quartz clustered | 3/5 ‚≠ê‚≠ê‚≠ê | 2/5 ‚≠ê‚≠ê | 5/5 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 4/5 ‚≠ê‚≠ê‚≠ê‚≠ê |
| Queue-based | 3/5 ‚≠ê‚≠ê‚≠ê | 2/5 ‚≠ê‚≠ê | 5/5 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 5/5 ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

---

**Next**: [02-Implementation-Approaches.md](02-Implementation-Approaches.md) - See real code for all 5 production approaches.

**Previous**: [00-Overview.md](00-Overview.md)
